---
title: "Introduction"
author: "Tim Trice"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      include = TRUE, 
                      fig.width = 7, 
                      fig.aspect = 0.5)
```

```{r, message = FALSE}
library(dplyr)
library(ggplot2)
library(NCDCStormEvents)
library(maps)
```

`NCDCStormEvents` is a library that accesses the [Storm Events Database](https://www.ncdc.noaa.gov/stormevents/details.jsp). The database contains three core datasets: `details`, `fatalities` and `locations`. They contain storm-related data since 1951 and, as of this writing, through July, 2016. 

The datasets are stored in gz format. `NCDCStormEvents` reads in the data as requested and can either return the dataset requested in its "raw" form or clean the data up a bit. 

We can start off by obtaining a full list of the datasets available with `get_listings()`.

```{r}
datasets <- get_listings()
head(datasets)
```

  * Name: file name of the gz archive
  
  * Modified: Last date modified
  
  * Size: file size in bytes
  
  * Type: details, fatalities or locations
  
  * Year: The year to which the dataset belongs.

Here is a breakdown of each dataset by file size.

```{r}
ggplot(datasets, aes(x = factor(Year), y = Size, fill = factor(Dataset))) + 
    scale_y_continuous(label = function(x){x * 10^-6}) + 
    ggtitle("Details, Fatalities and Locations by Year, Size") + 
    ylab("Size (MB)") + 
    scale_x_discrete(breaks = c(seq(1950, 2015, by = 10))) + 
    geom_bar(stat = "identity", position = position_dodge()) + 
    facet_grid(Dataset ~ ., scales = "free_y")
```

Both `details` and `fatalities` have data going back to 1951. `locations` only dates back to 1996. It is at this time the file sizes of our datasets jump significantly compared to previous years and `details` regularly exceeds several megabytes. 

A [codebook](http://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Export-Format.docx) is available for the datasets. However, it is incomplete and I have tried to explain the variables in more detail in the help documents. 

## Basic Usage

Get all datasets for 2015:

```{r, eval = FALSE}
get_data(year = 2015)
```

Get `details` and `fatatities` only.

```{r, eval = FALSE}
get_data(year = 2015, ds = c("details", "fatalities"))
```

`get_data` is a wrapper function to pull the datasets. If we only want one of the three datasets we can run any one of these three functions:

```{r, eval = FALSE}
get_details(2015)
get_fatalities(2015)
get_locations(2015)
```

Each of the above functions has a `clean` parameter that is set to TRUE by default.

The dataframes are brought to the global environment under the names `c.details`, `c.fatalitites` and `c.locations`; the 'c.' prefix indicating "clean". One additional dataset is created: `details_event_types`. Some rows in `details` have multiple event types so to keep only one `EVENT_TYPE` per row that variable is separated to keep `details$EVENT_ID` unique.

### Dirty Data

If you would like the raw data as-is set the `clean` parameter to FALSE.

```{r, eval = FALSE}
get_data(year = 2015, type = "details", clean = FALSE)
get_fatalities(year = 2015, clean = FALSE)
```

Datasets returned will be named `details`, `fatalities` or `locations`.

The raw datasets have numerous issues that would not fly (or should not) in real-world environments. Some of the issues are:

  * Redundant date/time variables
  
  * `EVENT_TYPE` does not meet NWS specifications (explained in help documentation).
  
  * String values where numeric/integer would be more efficient.

One example of the "dirty data" is `CZ_TIMEZONE` (when `clean` is TRUE this variable is removed).

```{r}
get_details(1951, clean = FALSE)
US <- map_data("state")
details %>% 
  select(CZ_TIMEZONE, BEGIN_LON, BEGIN_LAT) %>% 
  ggplot(aes(x = BEGIN_LON, y = BEGIN_LAT, colour = CZ_TIMEZONE)) + 
  geom_polygon(data = US, aes(x = long, y = lat, group = group), 
               colour = "grey10", fill = "white") + 
  geom_point(size = 1) + 
  labs(title = "CZ_TIMEZONE for `details` Dataset, 1951", x = "Longitude", y = "Latitude") + 
  theme_bw() + 
  coord_quickmap()
```

There are two glaring issues with this: obviously, CST (Central Standard Time) is listed for east and west coast timezones. Second, the 1951 dataset has data throughout the year including CDT (Central Daylight Time). Relying on `CZ_TIMEZONE` when cleaning date/time values would be a mistake.

So if you want to perform analysis you can ignore the `clean` parameter; otherwise if you are looking for a cleaning challenge set the parameter to TRUE.
